# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V3hVsMqutJkTX1_VJWlw9JHwvMHzWEGL

# Anti-cancer activity prediction

## Input: 
####It is a graph, with atoms representing nodes and bonds as edges.

## Output:
####It is a positive against non-small cell lung cancer, or negative otherwise. 
 

## Data mining function

#### Classification and prediction because we need to predict positive or negative.

## The challenges 

#### Upsampling data because the data imbalanced data and extract important features from data file and use some layers to prevent overfitting.

## Impact

#### Will this drug affect cancer or not?.

## Steps

#### 1- Reading data.
#### 2- Resampling data.
#### 3- Splitting the training set into train and test
#### 4- Visualizing/Inspecting a Sample
#### 5- Preprocessing(Tokenization, embedded and so on)      
#### 6- Trials


## Protocol 
#### I intend to use hold out method.


## Determine good/bad hyper-parameters
#### According to the accuracy graph if there is overfitting or underfitting and then I decided to change hyperparameters or add layers.
"""

import os
from google.colab import drive
drive.mount('/content/drive')

os.chdir('/content/drive/MyDrive/Colab_Notebooks/DM/Project_6')

"""##Import libraries"""

import tensorflow as tf
from tensorflow.math import segment_mean
from tensorflow import keras
from keras.regularizers import l2
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Embedding, Dense
from tensorflow.keras.optimizers import Adam

!pip install --quiet networkx
import networkx as nx
import matplotlib.pyplot as plt
from matplotlib import cm

import math
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import random

"""## Read SDF format data (structured-data format)"""

def read_sdf(file):
    with open(file, 'r') as rf:
        content = rf.read()
    samples = content.split('$$$$')       #Split in $$$$ that means the first sample is ended and so on
    
    #Extract important information from the file
    def parse_sample(s):
        lines = s.splitlines()
        links = []
        nodes = []
        label = 0       #Output(labels)
        for l in lines:
            if l.strip() == '1.0':
                label = 1
            if l.strip() == '-1.0':
                label = 0
            if l.startswith('    '):    #Split in each tab
                feature = l.split()
                node = feature[3]       #Node character(S,O,N,C)
                nodes.append(node)
            elif l.startswith(' '):
                lnk = l.split()
                # edge: (from, to,) (1-based index)
                if int(lnk[0]) - 1 < len(nodes):    #Linke between nodes
                    links.append((
                        int(lnk[0])-1, 
                        int(lnk[1])-1, # zero-based index
                        # int(lnk[2]) ignore edge weight
                    ))
        return nodes, np.array(links), label
    
    return [parse_sample(s) for s in tqdm(samples) if len(s[0]) > 0]

#Read training data
training_set = read_sdf('train.sdf')

#Reading testing data
testing_set  = read_sdf('test_x.sdf')

"""**Imbalanced data**

It refers to datasets in which the target class has an unequal distribution of observations, i.e. one class label has a large sample size while the other has a small number. So we can solved it by Resampling (Oversampling and Undersampling)
"""

#Check data balanced or not 
np.unique(np.array(training_set)[:,2],return_counts=True)

X_train = pd.DataFrame(np.array(training_set)[:,:-1])
y_train = pd.DataFrame(np.array(training_set)[:,2])

from sklearn.utils import resample

resampling = X_train.copy()
resampling[2] = y_train.values
class_0 = resampling[resampling[2]==0]
class_1 = resampling[resampling[2]==1]
class_1_resample = resample(class_1, replace=True, n_samples = 23806)
data_upsampling = pd.concat([class_0, class_1_resample])

training_set = data_upsampling.values.tolist()

np.unique(np.array(training_set)[:,2],return_counts=True)

#Splitting the training set into train and test
training_set, validation_set = train_test_split(training_set, test_size=0.15, random_state= 0)

"""## Visualizing/Inspecting a Sample"""

colors = cm.rainbow(np.linspace(0, 1, 50))

def visualize(sample):
    G=nx.Graph()
    nodes = sample[0]
    edges = sample[1]
    
    labeldict={}
    node_color=[]
    for i,n in enumerate(nodes):
        G.add_node(i)
        labeldict[i]=n
        node_color.append(colors[hash(n)%len(colors)])

    # a list of nodes:
    for e in edges:
        G.add_edge(e[0], e[1])
        
    nx.draw(G, labels=labeldict, with_labels = True, node_color = node_color)
    plt.show()
    
    return G

plt.clf()
visualize(training_set[0])

print('length of training_set:', len(training_set))
print('length of traiing_set[0]:', len(training_set[0]))
print('length of testing_set:', len(testing_set))
print('length of testing_set[0]:', len(testing_set[0]))

"""## Preprocessing

**Embedded**: is a machine learning algorithm that starts with random weights and learns an embedding for all of the words in the training dataset.

**Tokenizer** converts unique words into numbers and then apply this to all lines of data.

**Graph Neural Networks (GNNs)** are a subset of deep learning algorithms that do inference on data represented by graphs.
"""

max_vocab = 500       #Vocabular max number
max_len = 100

#Tokenizer converts unique characters into numbers
# build vocabulary from training set
all_nodes = [s[0] for s in training_set]
tokenizer = Tokenizer(num_words=max_vocab)
tokenizer.fit_on_texts(all_nodes)

def prepare_single_batch(samples):
    sample_nodes = [s[0] for s in samples]                              #s[0] ---> nodes
    sample_nodes = tokenizer.texts_to_sequences(sample_nodes)           #Convert characters into numbers
    sample_nodes = pad_sequences(sample_nodes, padding='post')          #Padding: to all samples have the same length
    max_nodes_len = np.shape(sample_nodes)[1]                           #shape: (num of samples, number of nodes)
    edges = [s[1]+i*max_nodes_len for i,s in enumerate(samples)]        #s[1]: links, i*max_nodes_len: to start indexing from current node not from 0 again
    edges = [e for e in edges if len(e) > 0]                            #make sure all elements are non-zero
    node_to_graph = [[i]*max_nodes_len for i in range(len(samples))]
    
    all_nodes = np.reshape(sample_nodes, -1)      #concatenate nodes of all samples together
    all_edges = np.concatenate(edges)             #concatenate edges of all samples together (because edge is tuple, doesnt hae reshape, output is 2D)

    node_to_graph = np.reshape(node_to_graph, -1)
    return {
        'data': all_nodes,
        'edges': all_edges,
        'node2grah': node_to_graph,
    }, np.array([s[2] for s in samples])



def gen_batch(dataset, batch_size=16, repeat=False, shuffle=True):        #To generate multiple batches
    while True:
        dataset = list(dataset)
        if shuffle:
            random.shuffle(dataset)
        l = len(dataset)
        for ndx in range(0, l, batch_size):
            batch_samples = dataset[ndx:min(ndx + batch_size, l)]
            yield prepare_single_batch(batch_samples)
        if not repeat:
            break

# showing one batch:
for train_batch in gen_batch(training_set, batch_size=4):
    for k,v in train_batch[0].items():          #k: labels, v: values
        print(k)
        print(v)
        #print(len(v))
        pass
    print('label', train_batch[1])
    break

!pip install --quiet tf2_gnn

# https://github.com/ /tf2-gnn
# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py

from tf2_gnn.layers.gnn import GNN, GNNInput

"""#Trials

## GNN: Graph Neural Networks are a general neural network architecture
It has Nodes v ∈ V take unique values and edges are pairs e = (v, v0) ∈ V × V
Graphs may also contain node labels lv ∈ {1, . . . , LV } for each node v and edge labels or edge types le ∈ {1, . . . , LE } for each edge.


### GGNN(Gated Graph Sequence Neural Networks): 
That is suitable for non-sequential outputs and it is used backpropagation through time in order to compute gradients. and it is unroll recurrence for a fixed number of steps and just use backpropagation through time with modern optimization methods. 


### RGCN(Relational Graph Convolutional Networks):
Encode multi-relational data. It is performed an aggregation of the node representation achieved by the GCN for each type of relation.

### RGAT(Relational Graph Attention Networks):
It uses gated skip connections to improve long-range modeling between nodes and uses a more scalable vector-based approach for parameterizing relations.

## Before Imbalanced data

**Trial_1**

Layer: GNN

without message_calculation_class

Score: 0.68685

---

**Trial_2**

Layer: GNN

without message_calculation_class

Score: 0.72763

---

## After solved Imbalanced data problem

**Trial_3**

Layer: GNN

without message_calculation_class

Score: 0.83173

---

**Trial_4**

Layer: GNN

Using message_calculation_class = 'GGNN'

Score: 0.84318

---

**Trial_5**

Layer: GNN

Using message_calculation_class = 'GGNN'

Score: 0.87102

---

**Trial_6**

Layer: GNN

Using message_calculation_class = 'RGCN'

Score: 0.82031

---

**Trial_7**

Layer: GNN

Using message_calculation_class = 'RGCN'

Score: 0.81744

---

**Trial_8**

Layer: GNN

Using message_calculation_class = 'RGCN'

Score: 0.80778

---

**Trial_9**

Layer: GNN

Using message_calculation_class = 'RGAT'

Score: 0.80779

---

**Trial_10**

Layer: GNN

Using message_calculation_class = 'RGAT'

Score: 0.82707

---

##Imbalanced data

### Trial_1
Layer: GNN

without message_calculation_class

I think the accuracy will be good because I did upsampling to the training data.
"""

data = keras.Input(batch_shape=(None,))           #Input to layer

# the first dim is different to the previous one. it is the total number of edges in this batch
edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)     #edge is 2D
node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Links
embeded = Embedding(tokenizer.num_words, 20)(data)

# number of graphs (number of samples)
num_graph = tf.reduce_max(node2graph)+1

gnn_input = GNNInput(
    node_features=embeded,
    adjacency_lists=(edge,),
    node_to_graph_map=node2graph, 
    num_graphs=num_graph,
)

# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py
params = GNN.get_default_hyperparameters()
params["hidden_dim"] = 32
gnn_layer = GNN(params)
gnn_out = gnn_layer(gnn_input)

print('gnn_out', gnn_out)


avg = segment_mean(                               #segment_mean: computes the mean along segments of a tensor.
    data=gnn_out,
    segment_ids=node2graph
)
print('mean:', avg)

pred = Dense(1, activation='sigmoid')(avg)        #binary classification: Dense(2), but here we only output 1 probability
print('pred:', pred)

model = Model(
    inputs={
        'data': data,
        'edges': edge,
        'node2grah': node2graph,
    },
    outputs=pred
)
model.summary()

# compile model with loss values for each task, AUC matrix 
model.compile(
    loss='BinaryCrossentropy',
    metrics=['AUC']
)

#Fit the model
batch_size = 16
num_batchs = math.ceil(len(training_set) / batch_size)
num_batchs_validation = math.ceil(len(validation_set) / batch_size)

history = model.fit(
    gen_batch(                #GNN model
        training_set, batch_size=batch_size, repeat=True
    ),
    steps_per_epoch=num_batchs,
    epochs=50,
    validation_data=gen_batch(
        validation_set, batch_size=50, repeat=True
    ),
    validation_steps=num_batchs_validation,
)

y_pred = model.predict(
    gen_batch(testing_set, batch_size=16, shuffle=False)
)

y_pred = np.reshape(y_pred, -1)
y_pred.shape

y_pred

# figure is used to create a new figure.
plt.figure(figsize=(15, 8))
plt.subplot(1, 2, 1)
plt.plot(history.history['auc'])
plt.plot(history.history['val_auc'])
plt.title('model auc') # graph title
plt.ylabel('Auc') # y axis lable
plt.xlabel('epoch') # x axis lable
plt.legend(['train', 'validation'], loc='upper left') # legend is an area describing the elements of the graph
plt.show()
plt.grid(True) # grid is the axis object's method toggles the visibility of the grid inside the figure.
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')  # graph title
plt.ylabel('loss') # y axis lable
plt.xlabel('epoch')# x axis lable
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd 
submission = pd.DataFrame({'label':y_pred})
submission.index.name = 'id'
submission.to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_6/sample_submission_1.csv')

"""Private score: 0.68685

### Trial_2
Layer: GNN

without message_calculation_class

The accuracy very bad so I think the accuracy will be improved or increased if I will add dropout hyperprameter to reduce overfitting.
"""

data = keras.Input(batch_shape=(None,))           #Input to layer

# the first dim is different to the previous one. it is the total number of edges in this batch
edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)     #edge is 2D
node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Links
embeded = Embedding(tokenizer.num_words, 20)(data)

# number of graphs (number of samples)
num_graph = tf.reduce_max(node2graph)+1

gnn_input = GNNInput(
    node_features=embeded,
    adjacency_lists=(edge,),
    node_to_graph_map=node2graph, 
    num_graphs=num_graph,
)

# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py
params = GNN.get_default_hyperparameters()
params["hidden_dim"] = 32
params["layer_input_dropout_rate"] = 0.2
gnn_layer = GNN(params)
gnn_out = gnn_layer(gnn_input)

print('gnn_out', gnn_out)


avg = segment_mean(                               #segment_mean: computes the mean along segments of a tensor.
    data=gnn_out,
    segment_ids=node2graph
)
print('mean:', avg)

pred = Dense(1, activation='sigmoid')(avg)        #binary classification: Dense(2), but here we only output 1 probability
print('pred:', pred)

model = Model(
    inputs={
        'data': data,
        'edges': edge,
        'node2grah': node2graph,
    },
    outputs=pred
)
model.summary()

# compile model with loss values for each task, AUC matrix 
model.compile(
    loss='BinaryCrossentropy',
    metrics=['AUC']
)

#Fit the model
batch_size = 16
num_batchs = math.ceil(len(training_set) / batch_size)
num_batchs_validation = math.ceil(len(validation_set) / batch_size)

history = model.fit(
    gen_batch(                #GNN model
        training_set, batch_size=batch_size, repeat=True
    ),
    steps_per_epoch=num_batchs,
    epochs=50,
    validation_data=gen_batch(
        validation_set, batch_size=50, repeat=True
    ),
    validation_steps=num_batchs_validation,
)

y_pred = model.predict(
    gen_batch(testing_set, batch_size=16, shuffle=False)
)

y_pred = np.reshape(y_pred, -1)
y_pred.shape

y_pred

# figure is used to create a new figure.
plt.figure(figsize=(15, 8))
plt.subplot(1, 2, 1)
plt.plot(history.history['auc'])
plt.plot(history.history['val_auc'])
plt.title('model auc') # graph title
plt.ylabel('Auc') # y axis lable
plt.xlabel('epoch') # x axis lable
plt.legend(['train', 'validation'], loc='upper left') # legend is an area describing the elements of the graph
plt.show()
plt.grid(True) # grid is the axis object's method toggles the visibility of the grid inside the figure.
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')  # graph title
plt.ylabel('loss') # y axis lable
plt.xlabel('epoch')# x axis lable
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd 
submission = pd.DataFrame({'label':y_pred})
submission.index.name = 'id'
submission.to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_6/sample_submission_2.csv')

"""Private score: 0.72763

##After solved Imbalanced data problem

### Trial_3
Layer: GNN

without message_calculation_class

I think the accuracy will be good because I did upsampling to the training data.
"""

data = keras.Input(batch_shape=(None,))           #Input to layer

# the first dim is different to the previous one. it is the total number of edges in this batch
edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)     #edge is 2D
node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Links
embeded = Embedding(tokenizer.num_words, 20)(data)

# number of graphs (number of samples)
num_graph = tf.reduce_max(node2graph)+1

gnn_input = GNNInput(
    node_features=embeded,
    adjacency_lists=(edge,),
    node_to_graph_map=node2graph, 
    num_graphs=num_graph,
)

# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py
params = GNN.get_default_hyperparameters()
params["hidden_dim"] = 32
gnn_layer = GNN(params)
gnn_out = gnn_layer(gnn_input)

print('gnn_out', gnn_out)


avg = segment_mean(                               #segment_mean: computes the mean along segments of a tensor.
    data=gnn_out,
    segment_ids=node2graph
)
print('mean:', avg)

pred = Dense(1, activation='sigmoid')(avg)        #binary classification: Dense(2), but here we only output 1 probability
print('pred:', pred)

model = Model(
    inputs={
        'data': data,
        'edges': edge,
        'node2grah': node2graph,
    },
    outputs=pred
)
model.summary()

# compile model with loss values for each task, AUC matrix 
model.compile(
    loss='BinaryCrossentropy',
    metrics=['AUC']
)

#Fit the model
batch_size = 16
num_batchs = math.ceil(len(training_set) / batch_size)
num_batchs_validation = math.ceil(len(validation_set) / batch_size)

history = model.fit(
    gen_batch(                #GNN model
        training_set, batch_size=batch_size, repeat=True
    ),
    steps_per_epoch=num_batchs,
    epochs=50,
    validation_data=gen_batch(
        validation_set, batch_size=50, repeat=True
    ),
    validation_steps=num_batchs_validation,
)

y_pred = model.predict(
    gen_batch(testing_set, batch_size=16, shuffle=False)
)

y_pred = np.reshape(y_pred, -1)
y_pred.shape

y_pred

# figure is used to create a new figure.
plt.figure(figsize=(15, 8))
plt.subplot(1, 2, 1)
plt.plot(history.history['auc'])
plt.plot(history.history['val_auc'])
plt.title('model auc') # graph title
plt.ylabel('Auc') # y axis lable
plt.xlabel('epoch') # x axis lable
plt.legend(['train', 'validation'], loc='upper left') # legend is an area describing the elements of the graph
plt.show()
plt.grid(True) # grid is the axis object's method toggles the visibility of the grid inside the figure.
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')  # graph title
plt.ylabel('loss') # y axis lable
plt.xlabel('epoch')# x axis lable
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd 
submission = pd.DataFrame({'label':y_pred})
submission.index.name = 'id'
submission.to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_6/sample_submission_3.csv')

"""Private score: 0.83173

### Trial_4
Layer: GNN

Using message_calculation_class = 'GGNN'

Accuracy in the previous trial may be good because Gating it makes the propagation model better. so I will try to use params['message_calculation_class'] to improve the accuracy, and I will change batch size. 

I will add early stopping to prevent overfitting.
"""

data = keras.Input(batch_shape=(None,))           #Input to layer

# the first dim is different to the previous one. it is the total number of edges in this batch
edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)     #edge is 2D
node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Links
embeded = Embedding(tokenizer.num_words, 20)(data)

# number of graphs (number of samples)
num_graph = tf.reduce_max(node2graph)+1

gnn_input = GNNInput(
    node_features=embeded,
    adjacency_lists=(edge,),
    node_to_graph_map=node2graph, 
    num_graphs=num_graph,
)

# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py
params = GNN.get_default_hyperparameters()
params["hidden_dim"] = 32
params['message_calculation_class'] = 'GGNN'
gnn_layer = GNN(params)
gnn_out = gnn_layer(gnn_input)

print('gnn_out', gnn_out)


avg = segment_mean(                               #segment_mean: computes the mean along segments of a tensor.
    data=gnn_out,
    segment_ids=node2graph
)
print('mean:', avg)

pred = Dense(1, activation='sigmoid')(avg)        #binary classification: Dense(2), but here we only output 1 probability
print('pred:', pred)

model = Model(
    inputs={
        'data': data,
        'edges': edge,
        'node2grah': node2graph,
    },
    outputs=pred
)
model.summary()

# compile model with loss values for each task, AUC matrix 
model.compile(
    loss='BinaryCrossentropy',
    metrics=['AUC']
)

#Fit the model
batch_size = 20
num_batchs = math.ceil(len(training_set) / batch_size)
num_batchs_validation = math.ceil(len(validation_set) / batch_size)

history = model.fit(
    gen_batch(                #GNN model
        training_set, batch_size=batch_size, repeat=True
    ),
    steps_per_epoch=num_batchs,
    epochs=50,
    validation_data=gen_batch(
        validation_set, batch_size=25, repeat=True
    ),
    validation_steps=num_batchs_validation, callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='auc', patience=5)
    ],
)

y_pred = model.predict(
    gen_batch(testing_set, batch_size=16, shuffle=False)
)

y_pred = np.reshape(y_pred, -1)
y_pred.shape

y_pred

# figure is used to create a new figure.
plt.figure(figsize=(15, 8))
plt.subplot(1, 2, 1)
plt.plot(history.history['auc'])
plt.plot(history.history['val_auc'])
plt.title('model auc') # graph title
plt.ylabel('Auc') # y axis lable
plt.xlabel('epoch') # x axis lable
plt.legend(['train', 'validation'], loc='upper left') # legend is an area describing the elements of the graph
plt.show()
plt.grid(True) # grid is the axis object's method toggles the visibility of the grid inside the figure.
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')  # graph title
plt.ylabel('loss') # y axis lable
plt.xlabel('epoch')# x axis lable
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd 
submission = pd.DataFrame({'label':y_pred})
submission.index.name = 'id'
submission.to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_6/sample_submission_4.csv')

"""Private score: 0.84318"""



"""### Trial_5
Layer: GNN

Using message_calculation_class = 'GGNN'

Accuracy in the previous trial improved so I will use layer_input_dropout_rate to improve the accuracy and prevent overfitting.
"""

data = keras.Input(batch_shape=(None,))           #Input to layer

# the first dim is different to the previous one. it is the total number of edges in this batch
edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)     #edge is 2D
node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Links
embeded = Embedding(tokenizer.num_words, 20)(data)

# number of graphs (number of samples)
num_graph = tf.reduce_max(node2graph)+1

gnn_input = GNNInput(
    node_features=embeded,
    adjacency_lists=(edge,),
    node_to_graph_map=node2graph, 
    num_graphs=num_graph,
)

# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py
params = GNN.get_default_hyperparameters()
params["hidden_dim"] = 32
params['message_calculation_class'] = 'GGNN'
params['layer_input_dropout_rate'] = 0.2
gnn_layer = GNN(params)
gnn_out = gnn_layer(gnn_input)

print('gnn_out', gnn_out)


avg = segment_mean(                               #segment_mean: computes the mean along segments of a tensor.
    data=gnn_out,
    segment_ids=node2graph
)
print('mean:', avg)

pred = Dense(1, activation='sigmoid')(avg)        #binary classification: Dense(2), but here we only output 1 probability
print('pred:', pred)

model = Model(
    inputs={
        'data': data,
        'edges': edge,
        'node2grah': node2graph,
    },
    outputs=pred
)
model.summary()

# compile model with loss values for each task, AUC matrix 
model.compile(
    loss='BinaryCrossentropy',
    metrics=['AUC']
)

#Fit the model
batch_size = 20
num_batchs = math.ceil(len(training_set) / batch_size)
num_batchs_validation = math.ceil(len(validation_set) / batch_size)

history = model.fit(
    gen_batch(                #GNN model
        training_set, batch_size=batch_size, repeat=True
    ),
    steps_per_epoch=num_batchs,
    epochs=50,
    validation_data=gen_batch(
        validation_set, batch_size=25, repeat=True
    ),
    validation_steps=num_batchs_validation, callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='auc', patience=5)
    ],
)

y_pred = model.predict(
    gen_batch(testing_set, batch_size=16, shuffle=False)
)

y_pred = np.reshape(y_pred, -1)
y_pred.shape

y_pred

# figure is used to create a new figure.
plt.figure(figsize=(15, 8))
plt.subplot(1, 2, 1)
plt.plot(history.history['auc'])
plt.plot(history.history['val_auc'])
plt.title('model auc') # graph title
plt.ylabel('Auc') # y axis lable
plt.xlabel('epoch') # x axis lable
plt.legend(['train', 'validation'], loc='upper left') # legend is an area describing the elements of the graph
plt.show()
plt.grid(True) # grid is the axis object's method toggles the visibility of the grid inside the figure.
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')  # graph title
plt.ylabel('loss') # y axis lable
plt.xlabel('epoch')# x axis lable
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd 
submission = pd.DataFrame({'label':y_pred})
submission.index.name = 'id'
submission.to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_6/sample_submission_5.csv')

"""Private score: 0.87102"""



"""### Trial_6
Layer: GNN

Using message_calculation_class = 'RGCN'

I will try to use different message_calculation_class with layer_input_dropout_rate to improve the accuracy and reduce the overfitting.
"""

data = keras.Input(batch_shape=(None,))           #Input to layer

# the first dim is different to the previous one. it is the total number of edges in this batch
edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)     #edge is 2D
node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Links
embeded = Embedding(tokenizer.num_words, 20)(data)

# number of graphs (number of samples)
num_graph = tf.reduce_max(node2graph)+1

gnn_input = GNNInput(
    node_features=embeded,
    adjacency_lists=(edge,),
    node_to_graph_map=node2graph, 
    num_graphs=num_graph,
)

# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py
params = GNN.get_default_hyperparameters()
params["hidden_dim"] = 32
params['message_calculation_class'] = 'RGCN'
params['layer_input_dropout_rate'] = 0.2          #Dropout to prevent overfitting
gnn_layer = GNN(params)
gnn_out = gnn_layer(gnn_input)

print('gnn_out', gnn_out)


avg = segment_mean(                               #segment_mean: computes the mean along segments of a tensor.
    data=gnn_out,
    segment_ids=node2graph
)
print('mean:', avg)

pred = Dense(1, activation='sigmoid')(avg)        #binary classification: Dense(2), but here we only output 1 probability
print('pred:', pred)

model = Model(
    inputs={
        'data': data,
        'edges': edge,
        'node2grah': node2graph,
    },
    outputs=pred
)
model.summary()

# compile model with loss values for each task, AUC matrix 
model.compile(
    loss='BinaryCrossentropy',
    metrics=['AUC']
)

#Fit the model
batch_size = 20
num_batchs = math.ceil(len(training_set) / batch_size)
num_batchs_validation = math.ceil(len(validation_set) / batch_size)

history = model.fit(
    gen_batch(                #GNN model
        training_set, batch_size=batch_size, repeat=True
    ),
    steps_per_epoch=num_batchs,
    epochs=25,
    validation_data=gen_batch(
        validation_set, batch_size=25, repeat=True
    ),
    validation_steps=num_batchs_validation, callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='auc', patience=5)
    ],
)

y_pred = model.predict(
    gen_batch(testing_set, batch_size=16, shuffle=False)
)

y_pred = np.reshape(y_pred, -1)
y_pred.shape

y_pred

# figure is used to create a new figure.
plt.figure(figsize=(15, 8))
plt.subplot(1, 2, 1)
plt.plot(history.history['auc'])
plt.plot(history.history['val_auc'])
plt.title('model auc') # graph title
plt.ylabel('Auc') # y axis lable
plt.xlabel('epoch') # x axis lable
plt.legend(['train', 'validation'], loc='upper left') # legend is an area describing the elements of the graph
plt.show()
plt.grid(True) # grid is the axis object's method toggles the visibility of the grid inside the figure.
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')  # graph title
plt.ylabel('loss') # y axis lable
plt.xlabel('epoch')# x axis lable
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd 
submission = pd.DataFrame({'label':y_pred})
submission.index.name = 'id'
submission.to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_6/sample_submission_6.csv')

"""Private score: 0.82031"""



"""### Trial_7
Layer: GNN

Using message_calculation_class = 'RGCN'

According to the previous trial i will increase the epochs to train and I will change number of hidden dimension.
"""

data = keras.Input(batch_shape=(None,))           #Input to layer

# the first dim is different to the previous one. it is the total number of edges in this batch
edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)     #edge is 2D
node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Links
embeded = Embedding(tokenizer.num_words, 20)(data)

# number of graphs (number of samples)
num_graph = tf.reduce_max(node2graph)+1

gnn_input = GNNInput(
    node_features=embeded,
    adjacency_lists=(edge,),
    node_to_graph_map=node2graph, 
    num_graphs=num_graph,
)

# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py
params = GNN.get_default_hyperparameters()
params["hidden_dim"] = 32
params['message_calculation_class'] = 'RGCN'
params['layer_input_dropout_rate'] = 0.3          #Dropout to prevent overfitting
gnn_layer = GNN(params)
gnn_out = gnn_layer(gnn_input)

print('gnn_out', gnn_out)


avg = segment_mean(                               #segment_mean: computes the mean along segments of a tensor.
    data=gnn_out,
    segment_ids=node2graph
)
print('mean:', avg)

pred = Dense(1, activation='sigmoid')(avg)        #binary classification: Dense(2), but here we only output 1 probability
print('pred:', pred)

model = Model(
    inputs={
        'data': data,
        'edges': edge,
        'node2grah': node2graph,
    },
    outputs=pred
)
model.summary()

# compile model with loss values for each task, AUC matrix 
model.compile(
    loss='BinaryCrossentropy',
    metrics=['AUC']
)

#Fit the model
batch_size = 20
num_batchs = math.ceil(len(training_set) / batch_size)
num_batchs_validation = math.ceil(len(validation_set) / batch_size)

history = model.fit(
    gen_batch(                #GNN model
        training_set, batch_size=batch_size, repeat=True
    ),
    steps_per_epoch=num_batchs,
    epochs=25,
    validation_data=gen_batch(
        validation_set, batch_size=30, repeat=True
    ),
    validation_steps=num_batchs_validation, callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='auc', patience=5)
    ],
)

y_pred = model.predict(
    gen_batch(testing_set, batch_size=16, shuffle=False)
)

y_pred = np.reshape(y_pred, -1)
y_pred.shape

y_pred

# figure is used to create a new figure.
plt.figure(figsize=(15, 8))
plt.subplot(1, 2, 1)
plt.plot(history.history['auc'])
plt.plot(history.history['val_auc'])
plt.title('model auc') # graph title
plt.ylabel('Auc') # y axis lable
plt.xlabel('epoch') # x axis lable
plt.legend(['train', 'validation'], loc='upper left') # legend is an area describing the elements of the graph
plt.show()
plt.grid(True) # grid is the axis object's method toggles the visibility of the grid inside the figure.
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')  # graph title
plt.ylabel('loss') # y axis lable
plt.xlabel('epoch')# x axis lable
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd 
submission = pd.DataFrame({'label':y_pred})
submission.index.name = 'id'
submission.to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_6/sample_submission_7.csv')

"""Private score: 0.81744"""



"""### Trial_8
Layer: GNN

Using message_calculation_class = 'RGCN'

When I change dropout value from 0.2 to 0.3 the accuracy decreased so i will change it again into 0.2 and increase the epochs and change number of hidden dimension. I think the accuracy will be increased.
"""

data = keras.Input(batch_shape=(None,))           #Input to layer

# the first dim is different to the previous one. it is the total number of edges in this batch
edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)     #edge is 2D
node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Links
embeded = Embedding(tokenizer.num_words, 20)(data)

# number of graphs (number of samples)
num_graph = tf.reduce_max(node2graph)+1

gnn_input = GNNInput(
    node_features=embeded,
    adjacency_lists=(edge,),
    node_to_graph_map=node2graph, 
    num_graphs=num_graph,
)

# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py
params = GNN.get_default_hyperparameters()
params["hidden_dim"] = 32
params['message_calculation_class'] = 'RGCN'
params['layer_input_dropout_rate'] = 0.2          #Dropout to prevent overfitting
gnn_layer = GNN(params)
gnn_out = gnn_layer(gnn_input)

print('gnn_out', gnn_out)


avg = segment_mean(                               #segment_mean: computes the mean along segments of a tensor.
    data=gnn_out,
    segment_ids=node2graph
)
print('mean:', avg)

pred = Dense(1, activation='sigmoid')(avg)        #binary classification: Dense(2), but here we only output 1 probability
print('pred:', pred)

model = Model(
    inputs={
        'data': data,
        'edges': edge,
        'node2grah': node2graph,
    },
    outputs=pred
)
model.summary()

# compile model with loss values for each task, AUC matrix 
model.compile(
    loss='BinaryCrossentropy',
    metrics=['AUC']
)

#Fit the model
batch_size = 16
num_batchs = math.ceil(len(training_set) / batch_size)
num_batchs_validation = math.ceil(len(validation_set) / batch_size)

history = model.fit(
    gen_batch(                #GNN model
        training_set, batch_size=batch_size, repeat=True
    ),
    steps_per_epoch=num_batchs,
    epochs=50,
    validation_data=gen_batch(
        validation_set, batch_size=25, repeat=True
    ),
    validation_steps=num_batchs_validation, callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='auc', patience=5)
    ],
)

y_pred = model.predict(
    gen_batch(testing_set, batch_size=16, shuffle=False)
)

y_pred = np.reshape(y_pred, -1)
y_pred.shape

y_pred

# figure is used to create a new figure.
plt.figure(figsize=(15, 8))
plt.subplot(1, 2, 1)
plt.plot(history.history['auc'])
plt.plot(history.history['val_auc'])
plt.title('model auc') # graph title
plt.ylabel('Auc') # y axis lable
plt.xlabel('epoch') # x axis lable
plt.legend(['train', 'validation'], loc='upper left') # legend is an area describing the elements of the graph
plt.show()
plt.grid(True) # grid is the axis object's method toggles the visibility of the grid inside the figure.
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')  # graph title
plt.ylabel('loss') # y axis lable
plt.xlabel('epoch')# x axis lable
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd 
submission = pd.DataFrame({'label':y_pred})
submission.index.name = 'id'
submission.to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_6/sample_submission_8.csv')

"""Private score: 0.80778"""



"""### Trial_9
Layer: GNN

Using message_calculation_class = 'RGAT'

I will try to use different message_calculation_class with layer_input_dropout_rate to improve the accuracy and reduce the overfitting.

As RGAT uses gated skip connections to improve long-range modeling between nodes and uses a more scalable vector-based approach for parameterizing relations to avoid vanishing gradients, over-parameterization, and oversmoothing.  so I think it will be improve the accuracy.
"""

data = keras.Input(batch_shape=(None,))           #Input to layer

# the first dim is different to the previous one. it is the total number of edges in this batch
edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)     #edge is 2D
node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Links
embeded = Embedding(tokenizer.num_words, 20)(data)

# number of graphs (number of samples)
num_graph = tf.reduce_max(node2graph)+1

gnn_input = GNNInput(
    node_features=embeded,
    adjacency_lists=(edge,),
    node_to_graph_map=node2graph, 
    num_graphs=num_graph,
)

# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py
params = GNN.get_default_hyperparameters()
params["hidden_dim"] = 32
params['message_calculation_class'] = 'RGAT'
params['layer_input_dropout_rate'] = 0.2          #Dropout to prevent overfitting
params['num_heads'] = 2
gnn_layer = GNN(params)
gnn_out = gnn_layer(gnn_input)

print('gnn_out', gnn_out)


avg = segment_mean(                               #segment_mean: computes the mean along segments of a tensor.
    data=gnn_out,
    segment_ids=node2graph
)
print('mean:', avg)

pred = Dense(1, activation='sigmoid')(avg)        #binary classification: Dense(2), but here we only output 1 probability
print('pred:', pred)

model = Model(
    inputs={
        'data': data,
        'edges': edge,
        'node2grah': node2graph,
    },
    outputs=pred
)
model.summary()

# compile model with loss values for each task, AUC matrix 
model.compile(
    loss='BinaryCrossentropy',
    metrics=['AUC']
)

#Fit the model
batch_size = 20
num_batchs = math.ceil(len(training_set) / batch_size)
num_batchs_validation = math.ceil(len(validation_set) / batch_size)

history = model.fit(
    gen_batch(                #GNN model
        training_set, batch_size=batch_size, repeat=True
    ),
    steps_per_epoch=num_batchs,
    epochs=25,
    validation_data=gen_batch(
        validation_set, batch_size=25, repeat=True
    ),
    validation_steps=num_batchs_validation, callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='auc', patience=5)
    ],
)

y_pred = model.predict(
    gen_batch(testing_set, batch_size=16, shuffle=False)
)

y_pred = np.reshape(y_pred, -1)
y_pred.shape

y_pred

# figure is used to create a new figure.
plt.figure(figsize=(15, 8))
plt.subplot(1, 2, 1)
plt.plot(history.history['auc'])
plt.plot(history.history['val_auc'])
plt.title('model auc') # graph title
plt.ylabel('Auc') # y axis lable
plt.xlabel('epoch') # x axis lable
plt.legend(['train', 'validation'], loc='upper left') # legend is an area describing the elements of the graph
plt.show()
plt.grid(True) # grid is the axis object's method toggles the visibility of the grid inside the figure.
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')  # graph title
plt.ylabel('loss') # y axis lable
plt.xlabel('epoch')# x axis lable
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd 
submission = pd.DataFrame({'label':y_pred})
submission.index.name = 'id'
submission.to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_6/sample_submission_9.csv')

"""Private score: 0.80779"""



"""### Trial_10
Layer: GNN

Using message_calculation_class = 'RGAT'

I will try to use different message_calculation_class with layer_input_dropout_rate and global_exchange_mode to improve the accuracy.
"""

data = keras.Input(batch_shape=(None,))           #Input to layer

# the first dim is different to the previous one. it is the total number of edges in this batch
edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)     #edge is 2D
node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Links
embeded = Embedding(tokenizer.num_words, 20)(data)

# number of graphs (number of samples)
num_graph = tf.reduce_max(node2graph)+1

gnn_input = GNNInput(
    node_features=embeded,
    adjacency_lists=(edge,),
    node_to_graph_map=node2graph, 
    num_graphs=num_graph,
)

# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py
params = GNN.get_default_hyperparameters()
params["hidden_dim"] = 32
params['message_calculation_class'] = 'RGAT'
params['layer_input_dropout_rate'] = 0.2          #Dropout to prevent overfitting
params['num_heads'] = 2
params['global_exchange_mode'] = 'mean'
gnn_layer = GNN(params)
gnn_out = gnn_layer(gnn_input)

print('gnn_out', gnn_out)


avg = segment_mean(                               #segment_mean: computes the mean along segments of a tensor.
    data=gnn_out,
    segment_ids=node2graph
)
print('mean:', avg)

pred = Dense(1, activation='sigmoid')(avg)        #binary classification: Dense(2), but here we only output 1 probability
print('pred:', pred)

model = Model(
    inputs={
        'data': data,
        'edges': edge,
        'node2grah': node2graph,
    },
    outputs=pred
)
model.summary()

# compile model with loss values for each task, AUC matrix 
model.compile(
    loss='BinaryCrossentropy',
    metrics=['AUC']
)

#Fit the model
batch_size = 20
num_batchs = math.ceil(len(training_set) / batch_size)
num_batchs_validation = math.ceil(len(validation_set) / batch_size)

history = model.fit(
    gen_batch(                #GNN model
        training_set, batch_size=batch_size, repeat=True
    ),
    steps_per_epoch=num_batchs,
    epochs=50,
    validation_data=gen_batch(
        validation_set, batch_size=25, repeat=True
    ),
    validation_steps=num_batchs_validation, callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='auc', patience=5)
    ],
)

y_pred = model.predict(
    gen_batch(testing_set, batch_size=16, shuffle=False)
)

y_pred = np.reshape(y_pred, -1)
y_pred.shape

y_pred

# figure is used to create a new figure.
plt.figure(figsize=(15, 8))
plt.subplot(1, 2, 1)
plt.plot(history.history['auc'])
plt.plot(history.history['val_auc'])
plt.title('model auc') # graph title
plt.ylabel('Auc') # y axis lable
plt.xlabel('epoch') # x axis lable
plt.legend(['train', 'validation'], loc='upper left') # legend is an area describing the elements of the graph
plt.show()
plt.grid(True) # grid is the axis object's method toggles the visibility of the grid inside the figure.
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')  # graph title
plt.ylabel('loss') # y axis lable
plt.xlabel('epoch')# x axis lable
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd 
submission = pd.DataFrame({'label':y_pred})
submission.index.name = 'id'
submission.to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_6/sample_submission_10.csv')

"""Private score: 0.82707"""



"""#Questions

**Q1\ Based on the provided template, describe the format of the input file (sdf file).**

The input file is (SDF) structure data file. It store information on a molecule's chemical composition. and also It is about the positions of individual atoms in a chemical molecule as well as the connections between them. The expression "$$$" differentiates between different molecules.

Each sample/molecule begins with a header that describes the compound's name. Othe sections include details on the amount of atoms, the version number, and connections, among other things. The elements of the molecule are described in the atom block. The bond blocks describes the compound's bonding structure. Both of these blocks are used in this assignment to obtain information about the compound and save it as edges and nodes.

Data consists of many characters called nodes like N C S O. 
It consists of links between nodes like 1 5 which means the link between node number 1 with node number 5. The end of each sample has a label value.

**What are the input tensors to the neural network model (their meaning, not just symbol)? What is each of their dims and their meaning (e.g. batch_size)?**

Data consists of many characters called **nodes** like N C S O, and The shape for each batch is [batch_size*max_len_nodes].

It consists of links between nodes like 1 5 which means the link between node number 1 with node number 5 called **edges**. and The shape of edge is [sum_of_all_edges,2].

The end of each sample has a **label** value[1, -1].

**For each dim of gnn_out, what does it symbolize? For each dim of avg, what does it symbolize?**

**gnn_out**: 
[batch size node dimension, hidden layers] is the shape of the gnn_out, where batch size node dimension is the dimension of the tokenized vector for the entire batch is the dimension of the input data node vector. For each hidden layer, it represents the model's aggregation output.

**avg**:
The segmented mean of the gnn_out is calculated using the segmented ids. The output of gnn out is [tokenized vector dimension, hidden layers] for each sample in the batch size. The average tensor's final output is of the shape [batch size, hidden layer]. It's a method of gathering information for each sample and presenting it as mean data.

**What is the difference between segment_mean and tf.reduce_mean? For each dim of pred, what does it symbolize?**

![SegmentMean.png](data:image/png;base64,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)

reference: https://www.tensorflow.org/api_docs/python/tf/math/segment_mean


segment_mean: 
Computes mean value base on each segmant 

reduce_mean:
Computes the mean of elements through dimensions of a tensor.

For example:

ar = [[1, 2],
      [3, 4]]

 reduce_mean(ar, axis = 0) = [(3+1)/2.0 = 4, (2+4)/2.0 = 3]

 reduce_mean(ar, axis = 1) = [(2+1)/2.0 = 1.5, (3+4)/2.0 = 3.5]

**What is the motivation/theory/idea to use multiple gcn layers comparing to just one? How many layers were used in the template?**

The motivation used multiple graph convolution network. In the template I used default message passing method is rgcn (Graph convolution layers) and it has 4 layer . We can stack more layers to get a deeper GCN. A deep GCN (more than 7 layers)
"""



"""#References

* https://www.cs.toronto.edu/~yujiali/files/talks/iclr16_ggnn_talk.pdf

* https://towardsdatascience.com/graph-neural-networks-for-multi-relational-data-27968a2ed143

* https://openreview.net/forum?id=v-9E8egy_i

* https://www.tensorflow.org/api_docs/python/tf/math/segment_mean

* https://arxiv.org/pdf/1511.05493.pdf
"""

